{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first model\n",
    "## simle train and test data only  \n",
    "## random and grid search parameter tuning\n",
    "\n",
    "\n",
    "1. random search for RANDOM_MAX_EVALS times\n",
    "2. grid search around random search's best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_MAX_EVALS = 1\n",
    "GRID_MAX_EVALS = 1\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_CASH_balance.csv   bureau.csv\t\tinstallments_payments.csv\r\n",
      "application_test.csv   bureau_balance.csv\tprevious_application.csv\r\n",
      "application_train.csv  credit_card_balance.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.41715669631958"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time.time()\n",
    "train_df = pd.read_csv('../input/application_train.csv')\n",
    "test_df = pd.read_csv('../input/application_test.csv')\n",
    "time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  7.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.concat([train_df, test_df])\n",
    "n_train = len(train_df)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "categorical_feats = [f for f in all_df.columns if all_df[f].dtype == 'object']\n",
    "for col in tqdm(categorical_feats):\n",
    "    all_df[col] = all_df[col].astype('str')\n",
    "    le.fit(all_df[col])\n",
    "    all_df[col] = le.transform(all_df[col])\n",
    "\n",
    "train_df = all_df[:n_train]\n",
    "test_df = all_df[n_train:].drop('TARGET', axis=1)\n",
    "\n",
    "del all_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing data\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(train_df.drop('TARGET', axis = 1), train_df['TARGET'], test_size = 6000, random_state = 50)\n",
    "# Create a training and testing dataset\n",
    "train_set = lgb.Dataset(data = train_features, label = train_labels)\n",
    "test_set = lgb.Dataset(data = test_features, label = test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(hyperparameters, iteration):\n",
    "    \"\"\"Objective function for grid and random search. Returns\n",
    "       the cross validation score from a set of hyperparameters.\"\"\"\n",
    "    \n",
    "    # Number of estimators will be found using early stopping\n",
    "    if 'n_estimators' in hyperparameters.keys():\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "     # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n",
    "    \n",
    "    # results to retun\n",
    "    score = cv_results['auc-mean'][-1]\n",
    "    estimators = len(cv_results['auc-mean'])\n",
    "    hyperparameters['n_estimators'] = estimators \n",
    "    \n",
    "    return [score, hyperparameters, iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(param_rand, max_evals = RANDOM_MAX_EVALS):\n",
    "    \"\"\"Random search for hyperparameter optimization\"\"\"\n",
    "    \n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                                  index = list(range(RANDOM_MAX_EVALS)))\n",
    "    \n",
    "    # Keep searching until reach max evaluations\n",
    "    for i in range(RANDOM_MAX_EVALS):\n",
    "        \n",
    "        # Choose random hyperparameters\n",
    "        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_rand.items()}\n",
    "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
    "\n",
    "        # Evaluate randomly selected hyperparameters\n",
    "        eval_results = objective(hyperparameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "    \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_rand = {\n",
    "    'boosting_type': ['gbdt', 'goss', 'dart'],\n",
    "    'num_leaves': list(range(20, 150)),\n",
    "    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "    'subsample': list(np.linspace(0.5, 1, 100)),\n",
    "    'is_unbalance': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_st = time.time()\n",
    "random_results = random_search(param_rand)\n",
    "print(\"%f s\" % (time.time() - ra_st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_rand_best = random_results['params'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_rand_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'boosting_type': [param_rand_best['boosting_type']],\n",
    "    'num_leaves': list(range(param_rand_best['num_leaves'] - 10, param_rand_best['num_leaves'] +11, 10)),\n",
    "    'learning_rate': list(np.logspace(np.log10(param_rand_best['learning_rate'] * 0.9 ), np.log10(param_rand_best['learning_rate'] * 1.1), base = 10, num = 3)),\n",
    "    'subsample_for_bin': list(range(param_rand_best['subsample_for_bin'] - 20000, param_rand_best['subsample_for_bin'] + 20001, 20000)),\n",
    "    'min_child_samples': list(range(param_rand_best['min_child_samples'], 500, 3)),\n",
    "    'reg_alpha': list(np.linspace(param_rand_best['reg_alpha'] * 0.9, min(param_rand_best['reg_alpha'] * 1.1, 1.0), 3)),\n",
    "#     'reg_alpha': list(np.linspace(param_rand_best['reg_alpha'] * 0. 9, min(param_rand_best['reg_alpha'] * 1.1, 1.0), 3)),\n",
    "    'reg_lambda': list(np.linspace(param_rand_best['reg_lambda'] * 0.9, min(param_rand_best['reg_lambda'] * 1.1, 1.0), 3)),\n",
    "    'colsample_bytree': list(np.linspace(param_rand_best['colsample_bytree'] * 0.9, min(param_rand_best['colsample_bytree'] * 1.1, 1.0), 3)),\n",
    "    'subsample': list(np.linspace(param_rand_best['subsample'] * 0.9, min(param_rand_best['subsample'] * 0.9, 1.0), 3)),\n",
    "    'is_unbalance': [param_rand_best['is_unbalance']]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def grid_search(param_grid, max_evals = GRID_MAX_EVALS):\n",
    "    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n",
    "    \n",
    "    # Dataframe to store results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(GRID_MAX_EVALS)))\n",
    "    \n",
    "    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through every possible combination of hyperparameters\n",
    "    for v in itertools.product(*values):\n",
    "        \n",
    "        # Create a hyperparameter dictionary\n",
    "        hyperparameters = dict(zip(keys, v))\n",
    "        \n",
    "        # Set the subsample ratio accounting for boosting type\n",
    "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
    "        \n",
    "        # Evalute the hyperparameters\n",
    "        eval_results = objective(hyperparameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Normally would not limit iterations\n",
    "        if i > GRID_MAX_EVALS:\n",
    "            break\n",
    "       \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_st = time.time()\n",
    "grid_results = grid_search(param_grid)\n",
    "print(\"grid search %f s\" % (time.time() - gr_st))\n",
    "print('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\n",
    "print('\\nThe best hyperparameters were:')\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(grid_results.loc[0, 'params'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
