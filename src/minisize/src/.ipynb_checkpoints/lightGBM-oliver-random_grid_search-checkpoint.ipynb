{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first model\n",
    "## simle train and test data only  \n",
    "## random and grid search parameter tuning\n",
    "\n",
    "\n",
    "1. random search for RANDOM_MAX_EVALS times\n",
    "2. grid search around random search's best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_MAX_EVALS = 1\n",
    "GRID_MAX_EVALS = 1\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_CASH_balance.csv   bureau.csv\t\tinstallments_payments.csv\r\n",
      "application_test.csv   bureau_balance.csv\tprevious_application.csv\r\n",
      "application_train.csv  credit_card_balance.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_input():\n",
    "    buro_bal = pd.read_csv('../input/bureau_balance.csv')\n",
    "    print('Buro bal shape : ', buro_bal.shape)\n",
    "    \n",
    "    # 1hot encoding\n",
    "    print('transform to dummies')\n",
    "    buro_bal = pd.concat([buro_bal, pd.get_dummies(buro_bal.STATUS, prefix='buro_bal_status')], axis=1).drop('STATUS', axis=1)\n",
    "    \n",
    "    # 'SK_ID_BUREAU', 'MONTHS_BALANCE'をカウント\n",
    "    print('Counting buros')\n",
    "    buro_counts = buro_bal[['SK_ID_BUREAU', 'MONTHS_BALANCE']].groupby('SK_ID_BUREAU').count()\n",
    "    buro_bal['buro_count'] = buro_bal['SK_ID_BUREAU'].map(buro_counts['MONTHS_BALANCE'])\n",
    "    \n",
    "    # balance をaverage\n",
    "    print('averaging buro bal')\n",
    "    avg_buro_bal = buro_bal.groupby('SK_ID_BUREAU').mean()\n",
    "    \n",
    "    avg_buro_bal.columns = ['avg_buro_' + f_ for f_ in avg_buro_bal.columns]\n",
    "    del buro_bal\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Read Bureau')\n",
    "    buro = pd.read_csv('../input/bureau.csv')\n",
    "    \n",
    "    # bureau を 1hot encoding\n",
    "    print('Go to dummies')\n",
    "    buro_credit_active_dum = pd.get_dummies(buro.CREDIT_ACTIVE, prefix='ca_')\n",
    "    buro_credit_currency_dum = pd.get_dummies(buro.CREDIT_CURRENCY, prefix='cu_')\n",
    "    buro_credit_type_dum = pd.get_dummies(buro.CREDIT_TYPE, prefix='ty_')\n",
    "    \n",
    "    # 1hotをマージ\n",
    "    buro_full = pd.concat([buro, buro_credit_active_dum, buro_credit_currency_dum, buro_credit_type_dum], axis=1)\n",
    "    # buro_full.columns = ['buro_' + f_ for f_ in buro_full.columns]\n",
    "    \n",
    "    del buro_credit_active_dum, buro_credit_currency_dum, buro_credit_type_dum\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Merge with buro avg')\n",
    "    buro_full = buro_full.merge(right=avg_buro_bal.reset_index(), how='left', on='SK_ID_BUREAU', suffixes=('', '_bur_bal'))\n",
    "    \n",
    "    # breauの数のカウント\n",
    "    print('Counting buro per SK_ID_CURR')\n",
    "    nb_bureau_per_curr = buro_full[['SK_ID_CURR', 'SK_ID_BUREAU']].groupby('SK_ID_CURR').count()\n",
    "    buro_full['SK_ID_BUREAU'] = buro_full['SK_ID_CURR'].map(nb_bureau_per_curr['SK_ID_BUREAU'])\n",
    "    \n",
    "    print('Averaging bureau')\n",
    "    avg_buro = buro_full.groupby('SK_ID_CURR').mean()\n",
    "    print(avg_buro.head())\n",
    "    \n",
    "    del buro, buro_full\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Read prev')\n",
    "    prev = pd.read_csv('../input/previous_application.csv')\n",
    "    \n",
    "    prev_cat_features = [\n",
    "        f_ for f_ in prev.columns if prev[f_].dtype == 'object'\n",
    "    ]\n",
    "    \n",
    "    print('Go to dummies')\n",
    "    prev_dum = pd.DataFrame()\n",
    "    for f_ in prev_cat_features:\n",
    "        prev_dum = pd.concat([prev_dum, pd.get_dummies(prev[f_], prefix=f_).astype(np.uint8)], axis=1)\n",
    "    \n",
    "    prev = pd.concat([prev, prev_dum], axis=1)\n",
    "    \n",
    "    del prev_dum\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Counting number of Prevs')\n",
    "    nb_prev_per_curr = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    prev['SK_ID_PREV'] = prev['SK_ID_CURR'].map(nb_prev_per_curr['SK_ID_PREV'])\n",
    "    \n",
    "    print('Averaging prev')\n",
    "    avg_prev = prev.groupby('SK_ID_CURR').mean()\n",
    "    print(avg_prev.head())\n",
    "    del prev\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Reading POS_CASH')\n",
    "    pos = pd.read_csv('../input/POS_CASH_balance.csv')\n",
    "    \n",
    "    print('Go to dummies')\n",
    "    pos = pd.concat([pos, pd.get_dummies(pos['NAME_CONTRACT_STATUS'])], axis=1)\n",
    "    \n",
    "    print('Compute nb of prevs per curr')\n",
    "    nb_prevs = pos[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    pos['SK_ID_PREV'] = pos['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n",
    "    \n",
    "    print('Go to averages')\n",
    "    avg_pos = pos.groupby('SK_ID_CURR').mean()\n",
    "    \n",
    "    del pos, nb_prevs\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Reading CC balance')\n",
    "    cc_bal = pd.read_csv('../input/credit_card_balance.csv')\n",
    "    \n",
    "    print('Go to dummies')\n",
    "    cc_bal = pd.concat([cc_bal, pd.get_dummies(cc_bal['NAME_CONTRACT_STATUS'], prefix='cc_bal_status_')], axis=1)\n",
    "    \n",
    "    nb_prevs = cc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    cc_bal['SK_ID_PREV'] = cc_bal['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n",
    "    \n",
    "    print('Compute average')\n",
    "    avg_cc_bal = cc_bal.groupby('SK_ID_CURR').mean()\n",
    "    avg_cc_bal.columns = ['cc_bal_' + f_ for f_ in avg_cc_bal.columns]\n",
    "    \n",
    "    del cc_bal, nb_prevs\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Reading Installments')\n",
    "    inst = pd.read_csv('../input/installments_payments.csv')\n",
    "    nb_prevs = inst[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    inst['SK_ID_PREV'] = inst['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n",
    "    \n",
    "    avg_inst = inst.groupby('SK_ID_CURR').mean()\n",
    "    avg_inst.columns = ['inst_' + f_ for f_ in avg_inst.columns]\n",
    "    \n",
    "    print('Read data and test')\n",
    "    data = pd.read_csv('../input/application_train.csv')\n",
    "    test = pd.read_csv('../input/application_test.csv')\n",
    "    print('Shapes : ', data.shape, test.shape)\n",
    "    \n",
    "    y = data['TARGET']\n",
    "    del data['TARGET']\n",
    "    \n",
    "    categorical_feats = [\n",
    "        f for f in data.columns if data[f].dtype == 'object'\n",
    "    ]\n",
    "    categorical_feats\n",
    "    for f_ in categorical_feats:\n",
    "        data[f_], indexer = pd.factorize(data[f_])\n",
    "        test[f_] = indexer.get_indexer(test[f_])\n",
    "        \n",
    "    data = data.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    data = data.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    data = data.merge(right=avg_pos.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_pos.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    data = data.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    data = data.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    del avg_buro, avg_prev\n",
    "    gc.collect()\n",
    "\n",
    "    return data, test, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "# Build model inputs\n",
    "data, test, y = build_model_input()\n",
    "\n",
    "# Split into training and testing data\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(data, y, test_size = 6000, random_state = 50)\n",
    "# Create a training and testing dataset\n",
    "train_set = lgb.Dataset(data = train_features, label = train_labels)\n",
    "test_set = lgb.Dataset(data = test_features, label = test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(hyperparameters, iteration):\n",
    "    \"\"\"Objective function for grid and random search. Returns\n",
    "       the cross validation score from a set of hyperparameters.\"\"\"\n",
    "    \n",
    "    # Number of estimators will be found using early stopping\n",
    "    if 'n_estimators' in hyperparameters.keys():\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "     # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n",
    "    \n",
    "    # results to retun\n",
    "    score = cv_results['auc-mean'][-1]\n",
    "    estimators = len(cv_results['auc-mean'])\n",
    "    hyperparameters['n_estimators'] = estimators \n",
    "    \n",
    "    return [score, hyperparameters, iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(param_rand, max_evals = RANDOM_MAX_EVALS):\n",
    "    \"\"\"Random search for hyperparameter optimization\"\"\"\n",
    "    \n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                                  index = list(range(RANDOM_MAX_EVALS)))\n",
    "    \n",
    "    # Keep searching until reach max evaluations\n",
    "    for i in tqdm(range(RANDOM_MAX_EVALS)):\n",
    "        \n",
    "        # Choose random hyperparameters\n",
    "        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_rand.items()}\n",
    "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
    "\n",
    "        # Evaluate randomly selected hyperparameters\n",
    "        eval_results = objective(hyperparameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "    \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_rand = {\n",
    "    'boosting_type': ['gbdt', 'goss', 'dart'],\n",
    "    'num_leaves': list(range(20, 150)),\n",
    "    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "    'subsample': list(np.linspace(0.5, 1, 100)),\n",
    "    'is_unbalance': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:29<00:00, 29.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.721659 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ra_st = time.time()\n",
    "random_results = random_search(param_rand)\n",
    "print(\"%f s\" % (time.time() - ra_st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>score</th>\n",
       "      <th>params</th>\n",
       "      <th>iteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.730809</td>\n",
       "      <td>{'boosting_type': 'dart', 'num_leaves': 50, 'l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     score                                             params  \\\n",
       "0      0  0.730809  {'boosting_type': 'dart', 'num_leaves': 50, 'l...   \n",
       "\n",
       "  iteration  \n",
       "0         0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_rand_best = random_results['params'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'dart',\n",
       " 'num_leaves': 50,\n",
       " 'learning_rate': 0.009800817371595928,\n",
       " 'subsample_for_bin': 20000,\n",
       " 'min_child_samples': 190,\n",
       " 'reg_alpha': 0.7142857142857142,\n",
       " 'reg_lambda': 0.16326530612244897,\n",
       " 'colsample_bytree': 0.6444444444444444,\n",
       " 'subsample': 0.5909090909090909,\n",
       " 'is_unbalance': False,\n",
       " 'metric': 'auc',\n",
       " 'verbose': 1,\n",
       " 'n_estimators': 105}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_rand_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'boosting_type': [param_rand_best['boosting_type']],\n",
    "    'num_leaves': list(range(max(1, param_rand_best['num_leaves'] - 10), param_rand_best['num_leaves'] +11, 10)),\n",
    "    'learning_rate': list(np.logspace(np.log10(param_rand_best['learning_rate'] * 0.9 ), np.log10(param_rand_best['learning_rate'] * 1.1), base = 10, num = 3)),\n",
    "    'subsample_for_bin': list(range(max(20000, param_rand_best['subsample_for_bin'] - 20000), param_rand_best['subsample_for_bin'] + 20001, 20000)),\n",
    "    'min_child_samples': list(range(param_rand_best['min_child_samples'], 500, 3)),\n",
    "    'reg_alpha': list(np.linspace(param_rand_best['reg_alpha'] * 0.9, min(param_rand_best['reg_alpha'] * 1.1, 1.0), 3)),\n",
    "    'reg_lambda': list(np.linspace(param_rand_best['reg_lambda'] * 0.9, min(param_rand_best['reg_lambda'] * 1.1, 1.0), 3)),\n",
    "    'colsample_bytree': list(np.linspace(param_rand_best['colsample_bytree'] * 0.9, min(param_rand_best['colsample_bytree'] * 1.1, 1.0), 3)),\n",
    "    'subsample': list(np.linspace(param_rand_best['subsample'] * 0.9, min(param_rand_best['subsample'] * 0.9, 1.0), 3)),\n",
    "    'is_unbalance': [param_rand_best['is_unbalance']]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def grid_search(param_grid, max_evals = GRID_MAX_EVALS):\n",
    "    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n",
    "    \n",
    "    # Dataframe to store results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(GRID_MAX_EVALS)))\n",
    "    \n",
    "    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through every possible combination of hyperparameters\n",
    "    for v in tqdm(itertools.product(*values)):\n",
    "        \n",
    "        # Create a hyperparameter dictionary\n",
    "        hyperparameters = dict(zip(keys, v))\n",
    "        \n",
    "        # Set the subsample ratio accounting for boosting type\n",
    "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
    "        \n",
    "        # Evalute the hyperparameters\n",
    "        eval_results = objective(hyperparameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Normally would not limit iterations\n",
    "        if i > GRID_MAX_EVALS:\n",
    "            break\n",
    "       \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:21, 21.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid search 42.820995 s\n",
      "The best validation score was 0.72831\n",
      "\n",
      "The best hyperparameters were:\n",
      "{'boosting_type': 'dart',\n",
      " 'colsample_bytree': 0.522,\n",
      " 'is_unbalance': False,\n",
      " 'learning_rate': 0.007938662070992697,\n",
      " 'metric': 'auc',\n",
      " 'min_child_samples': 190,\n",
      " 'n_estimators': 106,\n",
      " 'num_leaves': 30,\n",
      " 'reg_alpha': 0.5785714285714285,\n",
      " 'reg_lambda': 0.13224489795918368,\n",
      " 'subsample': 0.4786363636363637,\n",
      " 'subsample_for_bin': 20000,\n",
      " 'verbose': 1}\n"
     ]
    }
   ],
   "source": [
    "gr_st = time.time()\n",
    "grid_results = grid_search(param_grid)\n",
    "print(\"grid search %f s\" % (time.time() - gr_st))\n",
    "print('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\n",
    "print('\\nThe best hyperparameters were:')\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(grid_results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
