{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection \n",
    "reference : https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124.8741147518158"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time.time()\n",
    "df = pd.read_csv('./new_feature_by_kageyama_feature_matrix.csv')\n",
    "time.time() - st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK_ID_CURR\n",
      "STD(installments.SK_ID_CURR)\n",
      "MAX(installments.SK_ID_CURR)\n",
      "SKEW(installments.SK_ID_CURR)\n",
      "MIN(installments.SK_ID_CURR)\n",
      "MEAN(installments.SK_ID_CURR)\n",
      "STD(cash.SK_ID_CURR)\n",
      "MAX(cash.SK_ID_CURR)\n",
      "SKEW(cash.SK_ID_CURR)\n",
      "MIN(cash.SK_ID_CURR)\n",
      "MEAN(cash.SK_ID_CURR)\n",
      "STD(credit.SK_ID_CURR)\n",
      "MAX(credit.SK_ID_CURR)\n",
      "SKEW(credit.SK_ID_CURR)\n",
      "MIN(credit.SK_ID_CURR)\n",
      "MEAN(credit.SK_ID_CURR)\n",
      "STD(previous.MAX(installments.SK_ID_CURR))\n",
      "STD(previous.SKEW(installments.SK_ID_CURR))\n",
      "STD(previous.MIN(installments.SK_ID_CURR))\n",
      "STD(previous.MEAN(installments.SK_ID_CURR))\n",
      "STD(previous.MAX(cash.SK_ID_CURR))\n",
      "STD(previous.SKEW(cash.SK_ID_CURR))\n",
      "STD(previous.MIN(cash.SK_ID_CURR))\n",
      "STD(previous.MEAN(cash.SK_ID_CURR))\n",
      "STD(previous.MAX(credit.SK_ID_CURR))\n",
      "STD(previous.SKEW(credit.SK_ID_CURR))\n",
      "STD(previous.MIN(credit.SK_ID_CURR))\n",
      "STD(previous.MEAN(credit.SK_ID_CURR))\n",
      "MAX(previous.STD(installments.SK_ID_CURR))\n",
      "MAX(previous.SKEW(installments.SK_ID_CURR))\n",
      "MAX(previous.MIN(installments.SK_ID_CURR))\n",
      "MAX(previous.MEAN(installments.SK_ID_CURR))\n",
      "MAX(previous.STD(cash.SK_ID_CURR))\n",
      "MAX(previous.SKEW(cash.SK_ID_CURR))\n",
      "MAX(previous.MIN(cash.SK_ID_CURR))\n",
      "MAX(previous.MEAN(cash.SK_ID_CURR))\n",
      "MAX(previous.STD(credit.SK_ID_CURR))\n",
      "MAX(previous.SKEW(credit.SK_ID_CURR))\n",
      "MAX(previous.MIN(credit.SK_ID_CURR))\n",
      "MAX(previous.MEAN(credit.SK_ID_CURR))\n",
      "SKEW(previous.STD(installments.SK_ID_CURR))\n",
      "SKEW(previous.MAX(installments.SK_ID_CURR))\n",
      "SKEW(previous.MIN(installments.SK_ID_CURR))\n",
      "SKEW(previous.MEAN(installments.SK_ID_CURR))\n",
      "SKEW(previous.STD(cash.SK_ID_CURR))\n",
      "SKEW(previous.MAX(cash.SK_ID_CURR))\n",
      "SKEW(previous.MIN(cash.SK_ID_CURR))\n",
      "SKEW(previous.MEAN(cash.SK_ID_CURR))\n",
      "SKEW(previous.STD(credit.SK_ID_CURR))\n",
      "SKEW(previous.MAX(credit.SK_ID_CURR))\n",
      "SKEW(previous.MIN(credit.SK_ID_CURR))\n",
      "SKEW(previous.MEAN(credit.SK_ID_CURR))\n",
      "MIN(previous.STD(installments.SK_ID_CURR))\n",
      "MIN(previous.MAX(installments.SK_ID_CURR))\n",
      "MIN(previous.SKEW(installments.SK_ID_CURR))\n",
      "MIN(previous.MEAN(installments.SK_ID_CURR))\n",
      "MIN(previous.STD(cash.SK_ID_CURR))\n",
      "MIN(previous.MAX(cash.SK_ID_CURR))\n",
      "MIN(previous.SKEW(cash.SK_ID_CURR))\n",
      "MIN(previous.MEAN(cash.SK_ID_CURR))\n",
      "MIN(previous.STD(credit.SK_ID_CURR))\n",
      "MIN(previous.MAX(credit.SK_ID_CURR))\n",
      "MIN(previous.SKEW(credit.SK_ID_CURR))\n",
      "MIN(previous.MEAN(credit.SK_ID_CURR))\n",
      "MEAN(previous.STD(installments.SK_ID_CURR))\n",
      "MEAN(previous.MAX(installments.SK_ID_CURR))\n",
      "MEAN(previous.SKEW(installments.SK_ID_CURR))\n",
      "MEAN(previous.MIN(installments.SK_ID_CURR))\n",
      "MEAN(previous.MEAN(installments.SK_ID_CURR))\n",
      "MEAN(previous.STD(cash.SK_ID_CURR))\n",
      "MEAN(previous.MAX(cash.SK_ID_CURR))\n",
      "MEAN(previous.SKEW(cash.SK_ID_CURR))\n",
      "MEAN(previous.MIN(cash.SK_ID_CURR))\n",
      "MEAN(previous.MEAN(cash.SK_ID_CURR))\n",
      "MEAN(previous.STD(credit.SK_ID_CURR))\n",
      "MEAN(previous.MAX(credit.SK_ID_CURR))\n",
      "MEAN(previous.SKEW(credit.SK_ID_CURR))\n",
      "MEAN(previous.MIN(credit.SK_ID_CURR))\n",
      "MEAN(previous.MEAN(credit.SK_ID_CURR))\n"
     ]
    }
   ],
   "source": [
    "id_columns = []\n",
    "for c in df.columns:\n",
    "    if 'SK_ID' in c :\n",
    "        print(c)\n",
    "        if c != 'SK_ID_CURR':\n",
    "            id_columns.append(c)\n",
    "df = df.drop(id_columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('new_feature_by_kageyama_feature_matrix_noIDs.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set full shape:  (307507, 1352)\n",
      "Testing set full shape:  (48744, 1351)\n",
      "There are 266 columns with more than 75% missing values\n",
      "Training set full shape:  (307507, 1086)\n",
      "Testing set full shape:  (48744, 1085)\n"
     ]
    }
   ],
   "source": [
    "train_df = df.loc[df['set'] == 'train'] .drop('set', axis = 1)\n",
    "test_df = df.loc[df['set'] == 'test'] .drop(['set', 'TARGET'], axis=1)\n",
    "\n",
    "print('Training set full shape: ', train_df.shape)\n",
    "print('Testing set full shape: ' , test_df.shape)\n",
    "\n",
    "# Train missing values (in percent)\n",
    "train_missing = (train_df.isnull().sum() / len(train_df)).sort_values(ascending = False)\n",
    "train_missing.head()\n",
    "\n",
    "# Test missing values (in percent)\n",
    "test_missing = (test_df.isnull().sum() / len(test_df)).sort_values(ascending = False)\n",
    "test_missing.head()\n",
    "\n",
    "# Identify missing values above threshold\n",
    "train_missing = train_missing.index[train_missing > 0.75]\n",
    "test_missing = test_missing.index[test_missing > 0.75]\n",
    "\n",
    "all_missing = list(set(set(train_missing) | set(test_missing)))\n",
    "print('There are %d columns with more than 75%% missing values' % len(all_missing))\n",
    "\n",
    "# Need to save the labels because aligning will remove this column\n",
    "train_labels = train_df[\"TARGET\"]\n",
    "\n",
    "train_df = train_df.drop(columns = all_missing)\n",
    "test_df = test_df.drop(columns = all_missing)\n",
    "\n",
    "print('Training set full shape: ', train_df.shape)\n",
    "print('Testing set full shape: ' , test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(all_missing, axis = 1)\n",
    "df.to_csv('new_feature_by_kageyama_feature_matrix_noIDs_delmissing.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('SK_ID_CURR', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lable Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:42<00:00,  1.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df.loc[df['set'] == 'train'] .drop('set', axis = 1)\n",
    "test_df = df.loc[df['set'] == 'test'] .drop(['set', 'TARGET'], axis=1)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "categorical_feats = [f for f in df.columns if df[f].dtype == 'object']\n",
    "for col in tqdm(categorical_feats):\n",
    "    if col == 'set' :\n",
    "        continue\n",
    "    df[col] = df[col].astype('str')\n",
    "    le.fit(df[col])\n",
    "    df[col] = le.transform(df[col])\n",
    "train_df = df.loc[df['set'] == 'train'] .drop('set', axis = 1)\n",
    "test_df = df.loc[df['set'] == 'test'] .drop(['set', 'TARGET'], axis=1)\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape:  (307507, 1085)\n",
      "Testing shape:  (48744, 1084)\n"
     ]
    }
   ],
   "source": [
    "print('Training shape: ', train_df.shape)\n",
    "print('Testing shape: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds = np.zeros(train_df.shape[0])\n",
    "sub_preds = np.zeros(test_df.shape[0])\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "y = train_df['TARGET'].copy()\n",
    "X = train_df.drop('TARGET', axis = 1)\n",
    "\n",
    "\n",
    "feats =  list(X.columns)\n",
    "\n",
    "folds = KFold(n_splits=25, shuffle=True, random_state=2018)\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(X)):\n",
    "    train_X, train_y = X.iloc[trn_idx], y.iloc[trn_idx]\n",
    "    valid_X, valid_y = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "    clf = LGBMClassifier(\n",
    "        n_estimators=4000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=32,\n",
    "        colsample_bytree=.8,\n",
    "        subsample=.87,\n",
    "        max_depth=8,\n",
    "        reg_alpha=.0415,\n",
    "        reg_lambda=.0735,\n",
    "        min_split_gain=.02,\n",
    "        min_child_weight=40,\n",
    "        silent=-1,\n",
    "        verbose=-1,\n",
    "    )\n",
    "\n",
    "    clf.fit(train_X, train_y, \n",
    "            eval_set= [(train_X, train_y), (valid_X, valid_y)], \n",
    "            eval_metric='auc', verbose=100, early_stopping_rounds=100  #30\n",
    "           )\n",
    "\n",
    "    oof_preds[val_idx] = clf.predict_proba(valid_X, num_iteration=clf.best_iteration_)[:, 1]\n",
    "    sub_preds += clf.predict_proba(test_df, num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = feats\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "    fold_importance_df[\"fold\"] = n_fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[val_idx])))\n",
    "    del clf, train_X, train_y, valid_X, valid_y\n",
    "    gc.collect()\n",
    "\n",
    "print('Full AUC score %.6f' % roc_auc_score(y, oof_preds)) \n",
    "\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub['TARGET'] = sub_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('./kageyama_feature_matrix_25cv_tuned_by_Ivan_noIDs_delmissing_sub.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "def display_importances(feature_importance_df_):\n",
    "    # Plot feature importances\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "        by=\"importance\", ascending=False)[:50].index\n",
    "    \n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    \n",
    "    plt.figure(figsize=(8,10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", \n",
    "                data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_importances(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_feature_importances(df, threshold = 0.95):\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n",
    "    \n",
    "    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n",
    "    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_feature_importances = plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for cumulative importance\n",
    "threshold = 0.95\n",
    "\n",
    "# Extract the features to keep\n",
    "unimportant_features = list(norm_feature_importances[norm_feature_importances['cumulative_importance'] >= threshold]['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./new_feature_by_kageyama_feature_matrix_noIDs_delmissing.csv')\n",
    "df = df.drop(unimportant_features, axis=1)\n",
    "df.to_csv('./new_feature_by_kageyama_feature_matrix_noIDs_delmissing_delunimportant.csv',  index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
